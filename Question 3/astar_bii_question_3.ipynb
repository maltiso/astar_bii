{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A*Star BII programming interview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "hXmLYZFxDCmp",
    "outputId": "775feeb7-a48f-4eb7-8b75-8a129fd3d2c7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from keras.layers import Dense, Input\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vQcYK3UqDRx5"
   },
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(os.path.join('train_data.txt'), sep='\\t')\n",
    "y_train = pd.read_csv(os.path.join('train_truth.txt'), sep='\\t')\n",
    "\n",
    "X_test = pd.read_csv(os.path.join('test_data.txt'), sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V-bVVXdsbQ25"
   },
   "outputs": [],
   "source": [
    "lnr_model = LinearRegression()\n",
    "lnr_model.fit(X_train, y_train)\n",
    "y_pred = lnr_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "DQo_gAiBbQs1",
    "outputId": "c15d26d2-8f08-4cd4-f6d6-46c4895db01a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9844447735791826\n"
     ]
    }
   ],
   "source": [
    "print(r2_score(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A linear regressino model obtains an $R^2$ value of 0.9844, which indicates near perfect linear relationship. Since we do not have the truth for the test dataset, we will use this simple metric to judge that the fit is good. We now would like to build a neural network that can match or surpass this performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_CbLdpDqEych"
   },
   "outputs": [],
   "source": [
    "# Helper function for neural network experimentation\n",
    "def train_nn(x_train, y_train, input_layer, output_layer, \n",
    "             epochs, batch_size, lr, loss, metrics=None, \n",
    "             weights_file=None, history_file=None, reset=True):\n",
    "    # Initialize model\n",
    "    nn_model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    nn_model.summary()\n",
    "    # Load weights\n",
    "    if not reset and weights_file and os.path.isfile(weights_file):\n",
    "        try:\n",
    "            print('Weights loaded')\n",
    "            nn_model.load_weights(weights_file)\n",
    "        except:\n",
    "            pass\n",
    "    nn_model.compile(loss=loss, optimizer=Adam(lr=lr), metrics=metrics)\n",
    "    # Train model\n",
    "    start = time.time()\n",
    "    history = nn_model.fit(x_train, y_train, epochs=epochs, \n",
    "                           batch_size=batch_size, validation_split=0.2)\n",
    "    end = time.time()\n",
    "    print(end-start, 'secs')\n",
    "    # Add learning curve of current training session to history\n",
    "    if not reset and os.path.isfile(history_file):\n",
    "        with open(history_file, 'rb') as rf:\n",
    "            total_history = pickle.load(rf)\n",
    "    else:\n",
    "        total_history = None\n",
    "    if total_history:\n",
    "        for key in total_history.keys():\n",
    "            total_history[key] += history.history[key]\n",
    "    else:\n",
    "        total_history = history.history\n",
    "    # Save model\n",
    "    if weights_file:\n",
    "        nn_model.save_weights(weights_file)\n",
    "    if history_file:\n",
    "        pickle.dump(total_history, open(history_file, 'wb'))\n",
    "    return nn_model, total_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "MnFrYQPvDY0A",
    "outputId": "c68c1317-0a3a-48be-f635-db0a9eed937b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8)                 32        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 41\n",
      "Trainable params: 41\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/512\n",
      "8000/8000 [==============================] - 0s 37us/step - loss: 0.2042 - val_loss: 0.1664\n",
      "Epoch 2/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 0.1346 - val_loss: 0.1080\n",
      "Epoch 3/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 0.0870 - val_loss: 0.0705\n",
      "Epoch 4/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 0.0573 - val_loss: 0.0478\n",
      "Epoch 5/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 0.0397 - val_loss: 0.0346\n",
      "Epoch 6/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 0.0297 - val_loss: 0.0271\n",
      "Epoch 7/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 0.0240 - val_loss: 0.0228\n",
      "Epoch 8/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 0.0207 - val_loss: 0.0203\n",
      "Epoch 9/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 0.0187 - val_loss: 0.0185\n",
      "Epoch 10/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 0.0173 - val_loss: 0.0172\n",
      "Epoch 11/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 0.0161 - val_loss: 0.0161\n",
      "Epoch 12/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 0.0150 - val_loss: 0.0151\n",
      "Epoch 13/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 0.0140 - val_loss: 0.0141\n",
      "Epoch 14/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 0.0130 - val_loss: 0.0131\n",
      "Epoch 15/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 0.0121 - val_loss: 0.0122\n",
      "Epoch 16/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 0.0112 - val_loss: 0.0113\n",
      "Epoch 17/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 0.0104 - val_loss: 0.0105\n",
      "Epoch 18/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 0.0097 - val_loss: 0.0097\n",
      "Epoch 19/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 0.0089 - val_loss: 0.0090\n",
      "Epoch 20/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 0.0083 - val_loss: 0.0083\n",
      "Epoch 21/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 0.0076 - val_loss: 0.0077\n",
      "Epoch 22/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 0.0070 - val_loss: 0.0070\n",
      "Epoch 23/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 0.0064 - val_loss: 0.0064\n",
      "Epoch 24/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 0.0058 - val_loss: 0.0059\n",
      "Epoch 25/512\n",
      "8000/8000 [==============================] - 0s 5us/step - loss: 0.0053 - val_loss: 0.0053\n",
      "Epoch 26/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 0.0048 - val_loss: 0.0049\n",
      "Epoch 27/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 0.0044 - val_loss: 0.0044\n",
      "Epoch 28/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 0.0040 - val_loss: 0.0040\n",
      "Epoch 29/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 0.0036 - val_loss: 0.0037\n",
      "Epoch 30/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 0.0033 - val_loss: 0.0034\n",
      "Epoch 31/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 0.0031 - val_loss: 0.0031\n",
      "Epoch 32/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 0.0028 - val_loss: 0.0028\n",
      "Epoch 33/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 34/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 0.0024 - val_loss: 0.0024\n",
      "Epoch 35/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 36/512\n",
      "8000/8000 [==============================] - 0s 5us/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 37/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 38/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 39/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 40/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 41/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 42/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 43/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 44/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 45/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 46/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 47/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 48/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 49/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 50/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 51/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 52/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 53/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 9.9108e-04 - val_loss: 9.9847e-04\n",
      "Epoch 54/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 9.7734e-04 - val_loss: 9.8465e-04\n",
      "Epoch 55/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 9.6488e-04 - val_loss: 9.7048e-04\n",
      "Epoch 56/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 9.5278e-04 - val_loss: 9.5834e-04\n",
      "Epoch 57/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 9.4191e-04 - val_loss: 9.4586e-04\n",
      "Epoch 58/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 9.3115e-04 - val_loss: 9.3441e-04\n",
      "Epoch 59/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 9.2137e-04 - val_loss: 9.2384e-04\n",
      "Epoch 60/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 9.1195e-04 - val_loss: 9.1380e-04\n",
      "Epoch 61/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 9.0260e-04 - val_loss: 9.0426e-04\n",
      "Epoch 62/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 8.9414e-04 - val_loss: 8.9433e-04\n",
      "Epoch 63/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 8.8591e-04 - val_loss: 8.8544e-04\n",
      "Epoch 64/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 8.7751e-04 - val_loss: 8.7703e-04\n",
      "Epoch 65/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 8.6949e-04 - val_loss: 8.6788e-04\n",
      "Epoch 66/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 8.6133e-04 - val_loss: 8.5943e-04\n",
      "Epoch 67/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 8.5409e-04 - val_loss: 8.5142e-04\n",
      "Epoch 68/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 8.4651e-04 - val_loss: 8.4323e-04\n",
      "Epoch 69/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 8.3975e-04 - val_loss: 8.3574e-04\n",
      "Epoch 70/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 8.3255e-04 - val_loss: 8.2908e-04\n",
      "Epoch 71/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 8.2585e-04 - val_loss: 8.2083e-04\n",
      "Epoch 72/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 8.1911e-04 - val_loss: 8.1363e-04\n",
      "Epoch 73/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 8.1257e-04 - val_loss: 8.0673e-04\n",
      "Epoch 74/512\n",
      "8000/8000 [==============================] - 0s 5us/step - loss: 8.0639e-04 - val_loss: 8.0010e-04\n",
      "Epoch 75/512\n",
      "8000/8000 [==============================] - 0s 5us/step - loss: 8.0022e-04 - val_loss: 7.9335e-04\n",
      "Epoch 76/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 7.9426e-04 - val_loss: 7.8667e-04\n",
      "Epoch 77/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 7.8860e-04 - val_loss: 7.8034e-04\n",
      "Epoch 78/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 7.8292e-04 - val_loss: 7.7409e-04\n",
      "Epoch 79/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 7.7759e-04 - val_loss: 7.6773e-04\n",
      "Epoch 80/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 7.7176e-04 - val_loss: 7.6158e-04\n",
      "Epoch 81/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 7.6635e-04 - val_loss: 7.5568e-04\n",
      "Epoch 82/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 7.6100e-04 - val_loss: 7.4979e-04\n",
      "Epoch 83/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 7.5597e-04 - val_loss: 7.4438e-04\n",
      "Epoch 84/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 7.5102e-04 - val_loss: 7.3849e-04\n",
      "Epoch 85/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 7.4608e-04 - val_loss: 7.3315e-04\n",
      "Epoch 86/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 7.4125e-04 - val_loss: 7.2810e-04\n",
      "Epoch 87/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 7.3711e-04 - val_loss: 7.2380e-04\n",
      "Epoch 88/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 7.3267e-04 - val_loss: 7.1761e-04\n",
      "Epoch 89/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 7.2785e-04 - val_loss: 7.1276e-04\n",
      "Epoch 90/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 7.2298e-04 - val_loss: 7.0802e-04\n",
      "Epoch 91/512\n",
      "8000/8000 [==============================] - 0s 5us/step - loss: 7.1900e-04 - val_loss: 7.0306e-04\n",
      "Epoch 92/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 7.1470e-04 - val_loss: 6.9873e-04\n",
      "Epoch 93/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 7.1036e-04 - val_loss: 6.9450e-04\n",
      "Epoch 94/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 7.0627e-04 - val_loss: 6.9029e-04\n",
      "Epoch 95/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 7.0244e-04 - val_loss: 6.8531e-04\n",
      "Epoch 96/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 6.9915e-04 - val_loss: 6.8240e-04\n",
      "Epoch 97/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 6.9497e-04 - val_loss: 6.7748e-04\n",
      "Epoch 98/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 6.9141e-04 - val_loss: 6.7335e-04\n",
      "Epoch 99/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 6.8773e-04 - val_loss: 6.7101e-04\n",
      "Epoch 100/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 6.8412e-04 - val_loss: 6.6584e-04\n",
      "Epoch 101/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 6.8083e-04 - val_loss: 6.6273e-04\n",
      "Epoch 102/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 6.7732e-04 - val_loss: 6.5877e-04\n",
      "Epoch 103/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 6.7441e-04 - val_loss: 6.5523e-04\n",
      "Epoch 104/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 6.7105e-04 - val_loss: 6.5237e-04\n",
      "Epoch 105/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 6.6803e-04 - val_loss: 6.4921e-04\n",
      "Epoch 106/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 6.6529e-04 - val_loss: 6.4748e-04\n",
      "Epoch 107/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 6.6211e-04 - val_loss: 6.4299e-04\n",
      "Epoch 108/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 6.5945e-04 - val_loss: 6.4110e-04\n",
      "Epoch 109/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 6.5672e-04 - val_loss: 6.3737e-04\n",
      "Epoch 110/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 6.5357e-04 - val_loss: 6.3438e-04\n",
      "Epoch 111/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 6.5112e-04 - val_loss: 6.3278e-04\n",
      "Epoch 112/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 6.4877e-04 - val_loss: 6.2928e-04\n",
      "Epoch 113/512\n",
      "8000/8000 [==============================] - 0s 5us/step - loss: 6.4616e-04 - val_loss: 6.2636e-04\n",
      "Epoch 114/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 6.4337e-04 - val_loss: 6.2421e-04\n",
      "Epoch 115/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 6.4062e-04 - val_loss: 6.2125e-04\n",
      "Epoch 116/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 6.3841e-04 - val_loss: 6.1846e-04\n",
      "Epoch 117/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 6.3629e-04 - val_loss: 6.1668e-04\n",
      "Epoch 118/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 6.3353e-04 - val_loss: 6.1427e-04\n",
      "Epoch 119/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 6.3149e-04 - val_loss: 6.1179e-04\n",
      "Epoch 120/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 6.2895e-04 - val_loss: 6.0925e-04\n",
      "Epoch 121/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 6.2761e-04 - val_loss: 6.0779e-04\n",
      "Epoch 122/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 6.2478e-04 - val_loss: 6.0496e-04\n",
      "Epoch 123/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 6.2300e-04 - val_loss: 6.0366e-04\n",
      "Epoch 124/512\n",
      "8000/8000 [==============================] - 0s 5us/step - loss: 6.2116e-04 - val_loss: 6.0081e-04\n",
      "Epoch 125/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 6.1910e-04 - val_loss: 5.9908e-04\n",
      "Epoch 126/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 6.1753e-04 - val_loss: 5.9796e-04\n",
      "Epoch 127/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 6.1549e-04 - val_loss: 5.9560e-04\n",
      "Epoch 128/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 6.1383e-04 - val_loss: 5.9342e-04\n",
      "Epoch 129/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 6.1178e-04 - val_loss: 5.9129e-04\n",
      "Epoch 130/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 6.0988e-04 - val_loss: 5.9017e-04\n",
      "Epoch 131/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 6.0826e-04 - val_loss: 5.8803e-04\n",
      "Epoch 132/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 6.0661e-04 - val_loss: 5.8654e-04\n",
      "Epoch 133/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 6.0563e-04 - val_loss: 5.8440e-04\n",
      "Epoch 134/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 6.0365e-04 - val_loss: 5.8286e-04\n",
      "Epoch 135/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 6.0212e-04 - val_loss: 5.8168e-04\n",
      "Epoch 136/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 6.0046e-04 - val_loss: 5.8105e-04\n",
      "Epoch 137/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.9881e-04 - val_loss: 5.7914e-04\n",
      "Epoch 138/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.9740e-04 - val_loss: 5.7730e-04\n",
      "Epoch 139/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.9613e-04 - val_loss: 5.7630e-04\n",
      "Epoch 140/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.9475e-04 - val_loss: 5.7505e-04\n",
      "Epoch 141/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.9338e-04 - val_loss: 5.7396e-04\n",
      "Epoch 142/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.9205e-04 - val_loss: 5.7184e-04\n",
      "Epoch 143/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.9142e-04 - val_loss: 5.7269e-04\n",
      "Epoch 144/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.9043e-04 - val_loss: 5.7075e-04\n",
      "Epoch 145/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.8839e-04 - val_loss: 5.6848e-04\n",
      "Epoch 146/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.8744e-04 - val_loss: 5.6742e-04\n",
      "Epoch 147/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.8696e-04 - val_loss: 5.6595e-04\n",
      "Epoch 148/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.8528e-04 - val_loss: 5.6817e-04\n",
      "Epoch 149/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.8432e-04 - val_loss: 5.6498e-04\n",
      "Epoch 150/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.8300e-04 - val_loss: 5.6580e-04\n",
      "Epoch 151/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.8185e-04 - val_loss: 5.6251e-04\n",
      "Epoch 152/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.8142e-04 - val_loss: 5.6241e-04\n",
      "Epoch 153/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.8011e-04 - val_loss: 5.6189e-04\n",
      "Epoch 154/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.7954e-04 - val_loss: 5.5919e-04\n",
      "Epoch 155/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.7798e-04 - val_loss: 5.5821e-04\n",
      "Epoch 156/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.7668e-04 - val_loss: 5.5725e-04\n",
      "Epoch 157/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.7575e-04 - val_loss: 5.5651e-04\n",
      "Epoch 158/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.7506e-04 - val_loss: 5.5561e-04\n",
      "Epoch 159/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.7433e-04 - val_loss: 5.5523e-04\n",
      "Epoch 160/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.7349e-04 - val_loss: 5.5490e-04\n",
      "Epoch 161/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.7233e-04 - val_loss: 5.5296e-04\n",
      "Epoch 162/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.7155e-04 - val_loss: 5.5217e-04\n",
      "Epoch 163/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.7055e-04 - val_loss: 5.5617e-04\n",
      "Epoch 164/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.7163e-04 - val_loss: 5.5124e-04\n",
      "Epoch 165/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.7021e-04 - val_loss: 5.5036e-04\n",
      "Epoch 166/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.6886e-04 - val_loss: 5.4947e-04\n",
      "Epoch 167/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.6785e-04 - val_loss: 5.4911e-04\n",
      "Epoch 168/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.6698e-04 - val_loss: 5.4813e-04\n",
      "Epoch 169/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.6636e-04 - val_loss: 5.4868e-04\n",
      "Epoch 170/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.6633e-04 - val_loss: 5.4683e-04\n",
      "Epoch 171/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.6515e-04 - val_loss: 5.4617e-04\n",
      "Epoch 172/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.6465e-04 - val_loss: 5.4668e-04\n",
      "Epoch 173/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.6378e-04 - val_loss: 5.4498e-04\n",
      "Epoch 174/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.6333e-04 - val_loss: 5.4577e-04\n",
      "Epoch 175/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.6349e-04 - val_loss: 5.4467e-04\n",
      "Epoch 176/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.6305e-04 - val_loss: 5.4356e-04\n",
      "Epoch 177/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.6268e-04 - val_loss: 5.4284e-04\n",
      "Epoch 178/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.6149e-04 - val_loss: 5.4367e-04\n",
      "Epoch 179/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.6133e-04 - val_loss: 5.4593e-04\n",
      "Epoch 180/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.6167e-04 - val_loss: 5.4163e-04\n",
      "Epoch 181/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.6082e-04 - val_loss: 5.4151e-04\n",
      "Epoch 182/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.5999e-04 - val_loss: 5.4074e-04\n",
      "Epoch 183/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.5899e-04 - val_loss: 5.4009e-04\n",
      "Epoch 184/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.5905e-04 - val_loss: 5.3973e-04\n",
      "Epoch 185/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.5905e-04 - val_loss: 5.3908e-04\n",
      "Epoch 186/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.5902e-04 - val_loss: 5.3911e-04\n",
      "Epoch 187/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.5806e-04 - val_loss: 5.3834e-04\n",
      "Epoch 188/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.5767e-04 - val_loss: 5.3855e-04\n",
      "Epoch 189/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.5751e-04 - val_loss: 5.3828e-04\n",
      "Epoch 190/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.5743e-04 - val_loss: 5.3899e-04\n",
      "Epoch 191/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.5635e-04 - val_loss: 5.3841e-04\n",
      "Epoch 192/512\n",
      "8000/8000 [==============================] - 0s 5us/step - loss: 5.5630e-04 - val_loss: 5.3693e-04\n",
      "Epoch 193/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.5594e-04 - val_loss: 5.3646e-04\n",
      "Epoch 194/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.5597e-04 - val_loss: 5.3662e-04\n",
      "Epoch 195/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.5501e-04 - val_loss: 5.3599e-04\n",
      "Epoch 196/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.5457e-04 - val_loss: 5.3581e-04\n",
      "Epoch 197/512\n",
      "8000/8000 [==============================] - 0s 5us/step - loss: 5.5427e-04 - val_loss: 5.3538e-04\n",
      "Epoch 198/512\n",
      "8000/8000 [==============================] - 0s 5us/step - loss: 5.5399e-04 - val_loss: 5.3542e-04\n",
      "Epoch 199/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.5470e-04 - val_loss: 5.3509e-04\n",
      "Epoch 200/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.5354e-04 - val_loss: 5.3592e-04\n",
      "Epoch 201/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.5411e-04 - val_loss: 5.3452e-04\n",
      "Epoch 202/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.5300e-04 - val_loss: 5.3438e-04\n",
      "Epoch 203/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.5273e-04 - val_loss: 5.3442e-04\n",
      "Epoch 204/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.5278e-04 - val_loss: 5.3385e-04\n",
      "Epoch 205/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.5207e-04 - val_loss: 5.3373e-04\n",
      "Epoch 206/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.5192e-04 - val_loss: 5.3441e-04\n",
      "Epoch 207/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.5156e-04 - val_loss: 5.3409e-04\n",
      "Epoch 208/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.5272e-04 - val_loss: 5.3311e-04\n",
      "Epoch 209/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.5244e-04 - val_loss: 5.3443e-04\n",
      "Epoch 210/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.5137e-04 - val_loss: 5.3300e-04\n",
      "Epoch 211/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.5184e-04 - val_loss: 5.3603e-04\n",
      "Epoch 212/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.5152e-04 - val_loss: 5.3412e-04\n",
      "Epoch 213/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.5072e-04 - val_loss: 5.3326e-04\n",
      "Epoch 214/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.5026e-04 - val_loss: 5.3201e-04\n",
      "Epoch 215/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4990e-04 - val_loss: 5.3145e-04\n",
      "Epoch 216/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.4960e-04 - val_loss: 5.3255e-04\n",
      "Epoch 217/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.4973e-04 - val_loss: 5.3239e-04\n",
      "Epoch 218/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.4969e-04 - val_loss: 5.3140e-04\n",
      "Epoch 219/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4915e-04 - val_loss: 5.3083e-04\n",
      "Epoch 220/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.4909e-04 - val_loss: 5.3146e-04\n",
      "Epoch 221/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4997e-04 - val_loss: 5.3151e-04\n",
      "Epoch 222/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.4992e-04 - val_loss: 5.3230e-04\n",
      "Epoch 223/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.5125e-04 - val_loss: 5.3036e-04\n",
      "Epoch 224/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.5259e-04 - val_loss: 5.3590e-04\n",
      "Epoch 225/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4932e-04 - val_loss: 5.3041e-04\n",
      "Epoch 226/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4815e-04 - val_loss: 5.2982e-04\n",
      "Epoch 227/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4825e-04 - val_loss: 5.2957e-04\n",
      "Epoch 228/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.4812e-04 - val_loss: 5.3010e-04\n",
      "Epoch 229/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4813e-04 - val_loss: 5.3250e-04\n",
      "Epoch 230/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4844e-04 - val_loss: 5.2913e-04\n",
      "Epoch 231/512\n",
      "8000/8000 [==============================] - 0s 5us/step - loss: 5.4787e-04 - val_loss: 5.2902e-04\n",
      "Epoch 232/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4717e-04 - val_loss: 5.2899e-04\n",
      "Epoch 233/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4775e-04 - val_loss: 5.2926e-04\n",
      "Epoch 234/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4726e-04 - val_loss: 5.2924e-04\n",
      "Epoch 235/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4694e-04 - val_loss: 5.2933e-04\n",
      "Epoch 236/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4662e-04 - val_loss: 5.3034e-04\n",
      "Epoch 237/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.4689e-04 - val_loss: 5.3158e-04\n",
      "Epoch 238/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4706e-04 - val_loss: 5.2825e-04\n",
      "Epoch 239/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.4663e-04 - val_loss: 5.2819e-04\n",
      "Epoch 240/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.4631e-04 - val_loss: 5.2794e-04\n",
      "Epoch 241/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4574e-04 - val_loss: 5.2759e-04\n",
      "Epoch 242/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.4570e-04 - val_loss: 5.2791e-04\n",
      "Epoch 243/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.4682e-04 - val_loss: 5.2843e-04\n",
      "Epoch 244/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4657e-04 - val_loss: 5.2880e-04\n",
      "Epoch 245/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.4570e-04 - val_loss: 5.2808e-04\n",
      "Epoch 246/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4517e-04 - val_loss: 5.2879e-04\n",
      "Epoch 247/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4549e-04 - val_loss: 5.2749e-04\n",
      "Epoch 248/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4517e-04 - val_loss: 5.2743e-04\n",
      "Epoch 249/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4579e-04 - val_loss: 5.2714e-04\n",
      "Epoch 250/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.4524e-04 - val_loss: 5.2769e-04\n",
      "Epoch 251/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4474e-04 - val_loss: 5.2744e-04\n",
      "Epoch 252/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4462e-04 - val_loss: 5.2703e-04\n",
      "Epoch 253/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4518e-04 - val_loss: 5.2723e-04\n",
      "Epoch 254/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4488e-04 - val_loss: 5.2850e-04\n",
      "Epoch 255/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4545e-04 - val_loss: 5.2780e-04\n",
      "Epoch 256/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.4508e-04 - val_loss: 5.2708e-04\n",
      "Epoch 257/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4438e-04 - val_loss: 5.2755e-04\n",
      "Epoch 258/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.4424e-04 - val_loss: 5.2744e-04\n",
      "Epoch 259/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4473e-04 - val_loss: 5.2651e-04\n",
      "Epoch 260/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.4511e-04 - val_loss: 5.2639e-04\n",
      "Epoch 261/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4388e-04 - val_loss: 5.2622e-04\n",
      "Epoch 262/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4451e-04 - val_loss: 5.2630e-04\n",
      "Epoch 263/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4498e-04 - val_loss: 5.2663e-04\n",
      "Epoch 264/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4434e-04 - val_loss: 5.3258e-04\n",
      "Epoch 265/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.4429e-04 - val_loss: 5.2582e-04\n",
      "Epoch 266/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.4466e-04 - val_loss: 5.2859e-04\n",
      "Epoch 267/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.4456e-04 - val_loss: 5.2640e-04\n",
      "Epoch 268/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4336e-04 - val_loss: 5.2617e-04\n",
      "Epoch 269/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4456e-04 - val_loss: 5.2527e-04\n",
      "Epoch 270/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4329e-04 - val_loss: 5.2525e-04\n",
      "Epoch 271/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4370e-04 - val_loss: 5.2665e-04\n",
      "Epoch 272/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4307e-04 - val_loss: 5.2840e-04\n",
      "Epoch 273/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4405e-04 - val_loss: 5.2674e-04\n",
      "Epoch 274/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4412e-04 - val_loss: 5.2538e-04\n",
      "Epoch 275/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4347e-04 - val_loss: 5.2711e-04\n",
      "Epoch 276/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4332e-04 - val_loss: 5.2510e-04\n",
      "Epoch 277/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4391e-04 - val_loss: 5.2556e-04\n",
      "Epoch 278/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4388e-04 - val_loss: 5.2492e-04\n",
      "Epoch 279/512\n",
      "8000/8000 [==============================] - 0s 5us/step - loss: 5.4250e-04 - val_loss: 5.2599e-04\n",
      "Epoch 280/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4305e-04 - val_loss: 5.2842e-04\n",
      "Epoch 281/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4441e-04 - val_loss: 5.2499e-04\n",
      "Epoch 282/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4289e-04 - val_loss: 5.2536e-04\n",
      "Epoch 283/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.4284e-04 - val_loss: 5.2439e-04\n",
      "Epoch 284/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4243e-04 - val_loss: 5.2409e-04\n",
      "Epoch 285/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4340e-04 - val_loss: 5.2416e-04\n",
      "Epoch 286/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.4253e-04 - val_loss: 5.2745e-04\n",
      "Epoch 287/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.4332e-04 - val_loss: 5.2446e-04\n",
      "Epoch 288/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.4239e-04 - val_loss: 5.2470e-04\n",
      "Epoch 289/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.4255e-04 - val_loss: 5.2397e-04\n",
      "Epoch 290/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4243e-04 - val_loss: 5.2373e-04\n",
      "Epoch 291/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4237e-04 - val_loss: 5.2405e-04\n",
      "Epoch 292/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.4212e-04 - val_loss: 5.2498e-04\n",
      "Epoch 293/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4300e-04 - val_loss: 5.2770e-04\n",
      "Epoch 294/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4418e-04 - val_loss: 5.2955e-04\n",
      "Epoch 295/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4318e-04 - val_loss: 5.2454e-04\n",
      "Epoch 296/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4209e-04 - val_loss: 5.2502e-04\n",
      "Epoch 297/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4265e-04 - val_loss: 5.2408e-04\n",
      "Epoch 298/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4195e-04 - val_loss: 5.2355e-04\n",
      "Epoch 299/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4391e-04 - val_loss: 5.2549e-04\n",
      "Epoch 300/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.4286e-04 - val_loss: 5.2395e-04\n",
      "Epoch 301/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.4215e-04 - val_loss: 5.2344e-04\n",
      "Epoch 302/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4158e-04 - val_loss: 5.2289e-04\n",
      "Epoch 303/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4166e-04 - val_loss: 5.2305e-04\n",
      "Epoch 304/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4198e-04 - val_loss: 5.2358e-04\n",
      "Epoch 305/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.4169e-04 - val_loss: 5.2274e-04\n",
      "Epoch 306/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4202e-04 - val_loss: 5.2418e-04\n",
      "Epoch 307/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.4142e-04 - val_loss: 5.2402e-04\n",
      "Epoch 308/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.4158e-04 - val_loss: 5.2357e-04\n",
      "Epoch 309/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4126e-04 - val_loss: 5.2271e-04\n",
      "Epoch 310/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4084e-04 - val_loss: 5.2390e-04\n",
      "Epoch 311/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4153e-04 - val_loss: 5.2524e-04\n",
      "Epoch 312/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.4111e-04 - val_loss: 5.2262e-04\n",
      "Epoch 313/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4064e-04 - val_loss: 5.2287e-04\n",
      "Epoch 314/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4093e-04 - val_loss: 5.2310e-04\n",
      "Epoch 315/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4097e-04 - val_loss: 5.2174e-04\n",
      "Epoch 316/512\n",
      "8000/8000 [==============================] - 0s 5us/step - loss: 5.4063e-04 - val_loss: 5.2209e-04\n",
      "Epoch 317/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4077e-04 - val_loss: 5.2194e-04\n",
      "Epoch 318/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4049e-04 - val_loss: 5.2163e-04\n",
      "Epoch 319/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4022e-04 - val_loss: 5.2153e-04\n",
      "Epoch 320/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4036e-04 - val_loss: 5.2193e-04\n",
      "Epoch 321/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4004e-04 - val_loss: 5.2138e-04\n",
      "Epoch 322/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4055e-04 - val_loss: 5.2104e-04\n",
      "Epoch 323/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4106e-04 - val_loss: 5.2498e-04\n",
      "Epoch 324/512\n",
      "8000/8000 [==============================] - 0s 5us/step - loss: 5.4322e-04 - val_loss: 5.2119e-04\n",
      "Epoch 325/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4023e-04 - val_loss: 5.2070e-04\n",
      "Epoch 326/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.3931e-04 - val_loss: 5.2660e-04\n",
      "Epoch 327/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4151e-04 - val_loss: 5.2393e-04\n",
      "Epoch 328/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4033e-04 - val_loss: 5.2349e-04\n",
      "Epoch 329/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.3993e-04 - val_loss: 5.2108e-04\n",
      "Epoch 330/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.3905e-04 - val_loss: 5.2047e-04\n",
      "Epoch 331/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.3885e-04 - val_loss: 5.2018e-04\n",
      "Epoch 332/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.3948e-04 - val_loss: 5.2013e-04\n",
      "Epoch 333/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.3971e-04 - val_loss: 5.1952e-04\n",
      "Epoch 334/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.4097e-04 - val_loss: 5.2051e-04\n",
      "Epoch 335/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.3918e-04 - val_loss: 5.2055e-04\n",
      "Epoch 336/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.3885e-04 - val_loss: 5.1914e-04\n",
      "Epoch 337/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.3760e-04 - val_loss: 5.1924e-04\n",
      "Epoch 338/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.3742e-04 - val_loss: 5.2341e-04\n",
      "Epoch 339/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.3887e-04 - val_loss: 5.2121e-04\n",
      "Epoch 340/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.3688e-04 - val_loss: 5.1768e-04\n",
      "Epoch 341/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.3682e-04 - val_loss: 5.1987e-04\n",
      "Epoch 342/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.3807e-04 - val_loss: 5.2738e-04\n",
      "Epoch 343/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.3891e-04 - val_loss: 5.2622e-04\n",
      "Epoch 344/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.3645e-04 - val_loss: 5.1744e-04\n",
      "Epoch 345/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.3540e-04 - val_loss: 5.2174e-04\n",
      "Epoch 346/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.3667e-04 - val_loss: 5.2461e-04\n",
      "Epoch 347/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.3577e-04 - val_loss: 5.1839e-04\n",
      "Epoch 348/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.3452e-04 - val_loss: 5.1659e-04\n",
      "Epoch 349/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.3372e-04 - val_loss: 5.1640e-04\n",
      "Epoch 350/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.3311e-04 - val_loss: 5.1493e-04\n",
      "Epoch 351/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.3210e-04 - val_loss: 5.1497e-04\n",
      "Epoch 352/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.3176e-04 - val_loss: 5.1436e-04\n",
      "Epoch 353/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.3119e-04 - val_loss: 5.1446e-04\n",
      "Epoch 354/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.3116e-04 - val_loss: 5.1284e-04\n",
      "Epoch 355/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.2973e-04 - val_loss: 5.1266e-04\n",
      "Epoch 356/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.2941e-04 - val_loss: 5.1260e-04\n",
      "Epoch 357/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.2867e-04 - val_loss: 5.1199e-04\n",
      "Epoch 358/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.2727e-04 - val_loss: 5.1193e-04\n",
      "Epoch 359/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.2698e-04 - val_loss: 5.1053e-04\n",
      "Epoch 360/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.2635e-04 - val_loss: 5.1074e-04\n",
      "Epoch 361/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.2557e-04 - val_loss: 5.0976e-04\n",
      "Epoch 362/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.2443e-04 - val_loss: 5.1157e-04\n",
      "Epoch 363/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.2367e-04 - val_loss: 5.0894e-04\n",
      "Epoch 364/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.2252e-04 - val_loss: 5.0787e-04\n",
      "Epoch 365/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.2138e-04 - val_loss: 5.0753e-04\n",
      "Epoch 366/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.2079e-04 - val_loss: 5.1245e-04\n",
      "Epoch 367/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.2093e-04 - val_loss: 5.0774e-04\n",
      "Epoch 368/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.1832e-04 - val_loss: 5.0461e-04\n",
      "Epoch 369/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.1819e-04 - val_loss: 5.0715e-04\n",
      "Epoch 370/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.1751e-04 - val_loss: 5.0423e-04\n",
      "Epoch 371/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.1716e-04 - val_loss: 5.0249e-04\n",
      "Epoch 372/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.1507e-04 - val_loss: 5.0034e-04\n",
      "Epoch 373/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.1295e-04 - val_loss: 5.0155e-04\n",
      "Epoch 374/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.1121e-04 - val_loss: 5.0317e-04\n",
      "Epoch 375/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.1036e-04 - val_loss: 4.9897e-04\n",
      "Epoch 376/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.0850e-04 - val_loss: 4.9606e-04\n",
      "Epoch 377/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.0625e-04 - val_loss: 4.9484e-04\n",
      "Epoch 378/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 5.0530e-04 - val_loss: 4.9525e-04\n",
      "Epoch 379/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.0521e-04 - val_loss: 4.9157e-04\n",
      "Epoch 380/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.0211e-04 - val_loss: 4.9096e-04\n",
      "Epoch 381/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 5.0144e-04 - val_loss: 4.9003e-04\n",
      "Epoch 382/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 4.9756e-04 - val_loss: 4.8703e-04\n",
      "Epoch 383/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 4.9555e-04 - val_loss: 4.8565e-04\n",
      "Epoch 384/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 4.9328e-04 - val_loss: 4.8471e-04\n",
      "Epoch 385/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 4.9106e-04 - val_loss: 4.7927e-04\n",
      "Epoch 386/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 4.8776e-04 - val_loss: 4.7907e-04\n",
      "Epoch 387/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 4.8584e-04 - val_loss: 4.7668e-04\n",
      "Epoch 388/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 4.8307e-04 - val_loss: 4.7077e-04\n",
      "Epoch 389/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 4.7973e-04 - val_loss: 4.6705e-04\n",
      "Epoch 390/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 4.7807e-04 - val_loss: 4.6267e-04\n",
      "Epoch 391/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 4.7571e-04 - val_loss: 4.5926e-04\n",
      "Epoch 392/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 4.7090e-04 - val_loss: 4.5573e-04\n",
      "Epoch 393/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 4.6703e-04 - val_loss: 4.5192e-04\n",
      "Epoch 394/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 4.6340e-04 - val_loss: 4.5039e-04\n",
      "Epoch 395/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 4.6019e-04 - val_loss: 4.4229e-04\n",
      "Epoch 396/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 4.5470e-04 - val_loss: 4.3708e-04\n",
      "Epoch 397/512\n",
      "8000/8000 [==============================] - 0s 5us/step - loss: 4.4868e-04 - val_loss: 4.3188e-04\n",
      "Epoch 398/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 4.4519e-04 - val_loss: 4.2747e-04\n",
      "Epoch 399/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 4.3754e-04 - val_loss: 4.1903e-04\n",
      "Epoch 400/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 4.3089e-04 - val_loss: 4.1220e-04\n",
      "Epoch 401/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 4.2434e-04 - val_loss: 4.0762e-04\n",
      "Epoch 402/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 4.1619e-04 - val_loss: 3.9853e-04\n",
      "Epoch 403/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 4.0942e-04 - val_loss: 3.9181e-04\n",
      "Epoch 404/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 4.0275e-04 - val_loss: 3.8625e-04\n",
      "Epoch 405/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 3.9623e-04 - val_loss: 3.8076e-04\n",
      "Epoch 406/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 3.8683e-04 - val_loss: 3.7343e-04\n",
      "Epoch 407/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 3.7966e-04 - val_loss: 3.6828e-04\n",
      "Epoch 408/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 3.7292e-04 - val_loss: 3.6069e-04\n",
      "Epoch 409/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 3.6536e-04 - val_loss: 3.5417e-04\n",
      "Epoch 410/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 3.5850e-04 - val_loss: 3.4702e-04\n",
      "Epoch 411/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 3.5212e-04 - val_loss: 3.4169e-04\n",
      "Epoch 412/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 3.4652e-04 - val_loss: 3.3430e-04\n",
      "Epoch 413/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 3.3991e-04 - val_loss: 3.2828e-04\n",
      "Epoch 414/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 3.3545e-04 - val_loss: 3.2437e-04\n",
      "Epoch 415/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 3.3060e-04 - val_loss: 3.2139e-04\n",
      "Epoch 416/512\n",
      "8000/8000 [==============================] - 0s 5us/step - loss: 3.2626e-04 - val_loss: 3.1245e-04\n",
      "Epoch 417/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 3.1955e-04 - val_loss: 3.0784e-04\n",
      "Epoch 418/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 3.1497e-04 - val_loss: 3.0493e-04\n",
      "Epoch 419/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 3.1068e-04 - val_loss: 2.9998e-04\n",
      "Epoch 420/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 3.0690e-04 - val_loss: 2.9575e-04\n",
      "Epoch 421/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 3.0221e-04 - val_loss: 2.9151e-04\n",
      "Epoch 422/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.9864e-04 - val_loss: 2.8812e-04\n",
      "Epoch 423/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.9664e-04 - val_loss: 2.8743e-04\n",
      "Epoch 424/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.9225e-04 - val_loss: 2.8294e-04\n",
      "Epoch 425/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.8948e-04 - val_loss: 2.7959e-04\n",
      "Epoch 426/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.8613e-04 - val_loss: 2.7736e-04\n",
      "Epoch 427/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.8341e-04 - val_loss: 2.7450e-04\n",
      "Epoch 428/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.8119e-04 - val_loss: 2.7206e-04\n",
      "Epoch 429/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.7828e-04 - val_loss: 2.7136e-04\n",
      "Epoch 430/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.7756e-04 - val_loss: 2.6783e-04\n",
      "Epoch 431/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.7364e-04 - val_loss: 2.6539e-04\n",
      "Epoch 432/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.7161e-04 - val_loss: 2.6315e-04\n",
      "Epoch 433/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.6912e-04 - val_loss: 2.6212e-04\n",
      "Epoch 434/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.6712e-04 - val_loss: 2.6002e-04\n",
      "Epoch 435/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.6483e-04 - val_loss: 2.5960e-04\n",
      "Epoch 436/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.6365e-04 - val_loss: 2.5623e-04\n",
      "Epoch 437/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.6123e-04 - val_loss: 2.5404e-04\n",
      "Epoch 438/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 2.5974e-04 - val_loss: 2.5391e-04\n",
      "Epoch 439/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.5805e-04 - val_loss: 2.5267e-04\n",
      "Epoch 440/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.5636e-04 - val_loss: 2.4923e-04\n",
      "Epoch 441/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 2.5461e-04 - val_loss: 2.4812e-04\n",
      "Epoch 442/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.5361e-04 - val_loss: 2.4748e-04\n",
      "Epoch 443/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.5177e-04 - val_loss: 2.4524e-04\n",
      "Epoch 444/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.5035e-04 - val_loss: 2.4378e-04\n",
      "Epoch 445/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.4915e-04 - val_loss: 2.4218e-04\n",
      "Epoch 446/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.4788e-04 - val_loss: 2.4088e-04\n",
      "Epoch 447/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.4714e-04 - val_loss: 2.3976e-04\n",
      "Epoch 448/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 2.4618e-04 - val_loss: 2.3924e-04\n",
      "Epoch 449/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 2.4489e-04 - val_loss: 2.3749e-04\n",
      "Epoch 450/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.4336e-04 - val_loss: 2.3645e-04\n",
      "Epoch 451/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.4301e-04 - val_loss: 2.3591e-04\n",
      "Epoch 452/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.4212e-04 - val_loss: 2.3472e-04\n",
      "Epoch 453/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.4072e-04 - val_loss: 2.3452e-04\n",
      "Epoch 454/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.3955e-04 - val_loss: 2.3234e-04\n",
      "Epoch 455/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.3873e-04 - val_loss: 2.3165e-04\n",
      "Epoch 456/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.3758e-04 - val_loss: 2.3112e-04\n",
      "Epoch 457/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.3718e-04 - val_loss: 2.3005e-04\n",
      "Epoch 458/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.3630e-04 - val_loss: 2.2939e-04\n",
      "Epoch 459/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.3531e-04 - val_loss: 2.2962e-04\n",
      "Epoch 460/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 2.3502e-04 - val_loss: 2.2797e-04\n",
      "Epoch 461/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 2.3397e-04 - val_loss: 2.2824e-04\n",
      "Epoch 462/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.3452e-04 - val_loss: 2.2761e-04\n",
      "Epoch 463/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.3326e-04 - val_loss: 2.2673e-04\n",
      "Epoch 464/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.3259e-04 - val_loss: 2.2518e-04\n",
      "Epoch 465/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 2.3164e-04 - val_loss: 2.2519e-04\n",
      "Epoch 466/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 2.3126e-04 - val_loss: 2.2467e-04\n",
      "Epoch 467/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.3063e-04 - val_loss: 2.2489e-04\n",
      "Epoch 468/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.3047e-04 - val_loss: 2.2769e-04\n",
      "Epoch 469/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 2.3035e-04 - val_loss: 2.2312e-04\n",
      "Epoch 470/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.2977e-04 - val_loss: 2.2362e-04\n",
      "Epoch 471/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.2909e-04 - val_loss: 2.2274e-04\n",
      "Epoch 472/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.2901e-04 - val_loss: 2.2147e-04\n",
      "Epoch 473/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.2819e-04 - val_loss: 2.2146e-04\n",
      "Epoch 474/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 2.2860e-04 - val_loss: 2.2632e-04\n",
      "Epoch 475/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 2.3004e-04 - val_loss: 2.2410e-04\n",
      "Epoch 476/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 2.2765e-04 - val_loss: 2.2065e-04\n",
      "Epoch 477/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 2.2772e-04 - val_loss: 2.2001e-04\n",
      "Epoch 478/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.2668e-04 - val_loss: 2.2035e-04\n",
      "Epoch 479/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.2645e-04 - val_loss: 2.1963e-04\n",
      "Epoch 480/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 2.2605e-04 - val_loss: 2.1963e-04\n",
      "Epoch 481/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 2.2604e-04 - val_loss: 2.1973e-04\n",
      "Epoch 482/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.2611e-04 - val_loss: 2.2070e-04\n",
      "Epoch 483/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.2581e-04 - val_loss: 2.1898e-04\n",
      "Epoch 484/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.2528e-04 - val_loss: 2.2128e-04\n",
      "Epoch 485/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.2605e-04 - val_loss: 2.1835e-04\n",
      "Epoch 486/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 2.2489e-04 - val_loss: 2.1792e-04\n",
      "Epoch 487/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.2486e-04 - val_loss: 2.1891e-04\n",
      "Epoch 488/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 2.2735e-04 - val_loss: 2.1827e-04\n",
      "Epoch 489/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 2.2499e-04 - val_loss: 2.1748e-04\n",
      "Epoch 490/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.2479e-04 - val_loss: 2.1858e-04\n",
      "Epoch 491/512\n",
      "8000/8000 [==============================] - 0s 5us/step - loss: 2.2438e-04 - val_loss: 2.1923e-04\n",
      "Epoch 492/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.2470e-04 - val_loss: 2.2196e-04\n",
      "Epoch 493/512\n",
      "8000/8000 [==============================] - 0s 5us/step - loss: 2.2716e-04 - val_loss: 2.1721e-04\n",
      "Epoch 494/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.2440e-04 - val_loss: 2.1857e-04\n",
      "Epoch 495/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.2372e-04 - val_loss: 2.1670e-04\n",
      "Epoch 496/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.2352e-04 - val_loss: 2.1732e-04\n",
      "Epoch 497/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.2364e-04 - val_loss: 2.1685e-04\n",
      "Epoch 498/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.2351e-04 - val_loss: 2.1680e-04\n",
      "Epoch 499/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.2344e-04 - val_loss: 2.1673e-04\n",
      "Epoch 500/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.2368e-04 - val_loss: 2.1786e-04\n",
      "Epoch 501/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.2429e-04 - val_loss: 2.1676e-04\n",
      "Epoch 502/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.2421e-04 - val_loss: 2.1659e-04\n",
      "Epoch 503/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 2.2357e-04 - val_loss: 2.2174e-04\n",
      "Epoch 504/512\n",
      "8000/8000 [==============================] - 0s 3us/step - loss: 2.2479e-04 - val_loss: 2.1976e-04\n",
      "Epoch 505/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.2363e-04 - val_loss: 2.1678e-04\n",
      "Epoch 506/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.2352e-04 - val_loss: 2.1741e-04\n",
      "Epoch 507/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.2328e-04 - val_loss: 2.1797e-04\n",
      "Epoch 508/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.2342e-04 - val_loss: 2.1660e-04\n",
      "Epoch 509/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.2293e-04 - val_loss: 2.1650e-04\n",
      "Epoch 510/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.2408e-04 - val_loss: 2.1728e-04\n",
      "Epoch 511/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.2290e-04 - val_loss: 2.1700e-04\n",
      "Epoch 512/512\n",
      "8000/8000 [==============================] - 0s 4us/step - loss: 2.2320e-04 - val_loss: 2.1803e-04\n",
      "16.240912437438965 secs\n"
     ]
    }
   ],
   "source": [
    "# Define Neural Network\n",
    "mlp_in = Input(shape=X_train.shape[1:])\n",
    "mlp_1 = Dense(units=8, activation='relu', kernel_regularizer=None)(mlp_in)\n",
    "mlp_out = Dense(units=1)(mlp_1)\n",
    "\n",
    "# Train Neural Network\n",
    "mlp_results = train_nn(x_train=X_train, y_train=y_train, \n",
    "                       input_layer=mlp_in, output_layer=mlp_out, \n",
    "                       epochs=2**9, batch_size=2**9, lr=1e-3, \n",
    "                       loss='mean_squared_error', \n",
    "                       weights_file='mlp_weights.h5', \n",
    "                       history_file='mlp_history.pkl', \n",
    "                       reset=True)\n",
    "mlp_model, mlp_history = mlp_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "colab_type": "code",
    "id": "XOKpmFK7E5g6",
    "outputId": "c9ff3ac9-0353-4da8-b78f-a191d0f6ec5a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f828d013438>"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(mlp_history['loss'])\n",
    "plt.plot(mlp_history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write output prediction\n",
    "y_test_pred = mlp_model.predict(X_test)\n",
    "pd.DataFrame({'y':y_test_pred[:, 0]}).to_csv('test_predicted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "rQQP58y9FnnV",
    "outputId": "51942a77-f532-4edc-9736-9a38f47d5803"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9935485578218093\n"
     ]
    }
   ],
   "source": [
    "y_pred = mlp_model.predict(X_train)\n",
    "print(r2_score(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fdJcXwDzG-c4"
   },
   "source": [
    "The multilayer perceptron obtains an $R^2$ of 0.9935 which is higher than that obtained by the linear regression model. This is a promising result.\n",
    "\n",
    "Based on the learning curve, there is no evidence of over-fitting, so we can expect the model to work well for unseen data."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
